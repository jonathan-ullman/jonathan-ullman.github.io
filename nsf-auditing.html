<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
   "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" lang="en">

<head>
	
	<title>
		Auditing Private Statistical and Machine Learning Algorithms: Theory and Practice
	</title>
	<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=Noto+Sans:ital,wght@0,400;0,700;1,400;1,700&family=Vollkorn&display=swap">
	<link rel="stylesheet" href="css/main.css" type="text/css"/>
</head>

<body>
    
<div id="contentpane">
    
    <div id="content">

    <div id="pagetitle"> 
		
    
    </div>
    

<div>
	
<h2>CNS-2247484 SaTC: CORE: Small: Auditing Private Statistical and Machine Learning Algorithms: Theory and Practice</h2>	
	
<h4>Project Abstract</h4>
	
	<p>The use of statistics and machine learning to analyze sensitive data poses serious privacy risks to users who contribute their data to these algorithms, including: data reconstruction (revealing large portions of the training data), membership inference (revealing the presence of specific individuals in the training data), and data memorization (revealing specific training examples). Differential privacy has now become the standard for protecting data privacy in machine learning and statistics, as it offers a strong, formal, and quantitative guarantee of individual privacy. However, for many deployments of differential privacy there remains a significant gap between the formal guarantees and expectations in practice, and this project aims to bridge this gap by auditing deployed algorithms to discover and explain their privacy properties. The project?s novelties are building the theoretical foundations of privacy auditing and designing empirical methods that measure the privacy leakage of private algorithms in real-world scenarios. The project?s broader significance and importance will be in specific recommendations to practitioners on choice and use of private algorithms, and an understanding of the potential privacy violations for specific machine learning applications. The project team has expertise in differential privacy, machine learning, and cybersecurity, and plans a set of education tasks and outreach activities: public course materials on trustworthy machine learning and privacy, mentoring undergraduate and graduate students in research projects, and collaboration with industry partners.</p>

	<p>This project includes three interconnected thrusts addressing different aspects of privacy auditing. The first thrust lays the theoretical foundation of privacy auditing by developing optimal membership inference attacks with stronger guarantees than previous work. The second thrust introduces novel methods for auditing convex machine learning models and neural networks, by using insights from poisoning attacks developed in adversarial machine learning. The final thrust designs tools for auditing end-to-end privacy leakage of machine learning models trained under the continual-learning paradigm. These research thrusts enable a suite of techniques for auditing private algorithms, with the goal of providing guidance to practitioners on how to select private algorithms and their parameters to balance the utility and privacy guarantees on tasks of interest.</p>
	
<h4>People</h4>

<ul>
	<li><a href="https://www.khoury.northeastern.edu/home/alina/">Alina Oprea</a>, Northeastern University, PI</li>
	<li><a href="https://jonathan-ullman.github.io/">Jonathan Ullman</a>, Northeastern University, PI</li>
	<li><a href="">John Abascal</a>, Northeastern University, PhD Student</li>
	<li><a href="">Harsh Chaudhari</a>, Northeastern University, PhD Student</li>
	<li><a href="">Lisa Oakley</a>, Northeastern University, PhD Student</li>
	<li><a href="">Giorgio Severi</a>, Northeastern University, PhD Student</li>
	<li><a href="">Nico Berrios</a>, Catholic University of Chile, Undergraduate</li>
	<li><a href="">Stanley Wu</a>, Northeastern University, Undergraduate (now a PhD student at University of Chicago)</li>
</ul>

<h4>Publications</h4>

<div class="paper">
	<strong>Metalearning with Very Few Samples Per Task</strong>
	<a href="https://arxiv.org/abs/2312.13978"><span id="label-preprint">arXiv</span></a><br/>
	<a href="https://www.mit.edu/~maryama/">Maryam Aliakbarpour</a>,
	<a href="https://konstantinabrk.github.io/">Konstantina Bairaktari</a>,	
	<a href="https://cs-people.bu.edu/grbrown/">Gavin Brown</a>, 
	<a href="https://cs-people.bu.edu/ads22/">Adam Smith</a>, and
	Jonathan Ullman<br/>
	Conference on Learning Theory <strong>(COLT '24)</strong><br/>
</div>

<div class="paper">
	<strong>TMI! Finetuned Models Leak Private Information from their Pretraining Data</strong>
	<a href="https://arxiv.org/abs/2306.01181"><span id="label-preprint">arXiv</span></a><br/>
	<a href="http://www.johnabascal.com/">John Abascal</a>,	
	<a href="">Stanley Wu</a>,
	<a href="https://www.ccs.neu.edu/home/alina/">Alina Oprea</a>, and
	Jonathan Ullman (contribution order)<br/>
	Privacy Enhancing Technologies Symposium <strong>(PETS '24)</strong><br/>
</div>

<div class="paper">
	<strong>Chameleon: Increasing Label-Only Membership Leakage with Adaptive Poisoning</strong>
	<a href="https://arxiv.org/abs/2310.03838"><span id="label-preprint">arXiv</span></a> <a href="https://github.com/harshch1803/CHAMELEON"><span id="label-code">GitHub</span></a> <a href="https://iclr.cc/virtual/2024/poster/19475"><span id="label-talk">Video</span></a><br/>
	<a href="https://harshch1803.github.io/">Harsh Chaudhuri</a>,	
	<a href="https://severi.xyz/">Giorgio Severi</a>, 
	<a href="https://www.ccs.neu.edu/home/alina/">Alina Oprea</a>, and
	Jonathan Ullman (contribution order)<br/>
	International Conference on Learning Representations <strong>(ICLR '24)</strong><br/>
</div>

<div class="paper">
	<strong>Synthesizing Tight Privacy and Accuracy Bounds via Weighted Model Counting</strong>
	<a href="https://arxiv.org/abs/2402.16982"><span id="label-preprint">arXiv</span></a><br/>
	<a href="https://harshch1803.github.io/">Lisa Oakley</a>,	
	<a href="https://severi.xyz/">Steven Holtzen</a>, and
	<a href="https://www.ccs.neu.edu/home/alina/">Alina Oprea</a><br/>
	IEEE Computer Security Foundations Symposium <strong>(CSF '24)</strong><br/>
</div>

</div>       

   


    </div>


</div>

    <div id="buffer"></div>
    <div id="footer">   <div>Template by Kira Goldner&#169;</div> </div>
    <!-- If you really feel uncomfortable with giving me credit by name on your site, rather than deleting, please at lease uncomment and use the following logo credit. -->
    <!--div id="footer"><div id="logo"> <a href="https://www.kiragoldner.com/" target="_blank"><img src="https://www.kiragoldner.com/favicon.png" width="12pt"/></a></div><div>&#169; Template</div-->

  </body>


</html>